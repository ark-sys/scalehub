---
apiVersion: v1
kind: ConfigMap
metadata:
  name: flink-config-jobmanager
  namespace: flink
  labels:
    app: flink
data:
  flink-conf-jobmanager.yaml: |+
    #scheduler-mode: reactive
    web.upload.dir: /tmp
    io.tmp.dirs: /tmp
    rest.profiling.enabled: true

    akka.framesize: 104857600b
    blob.server.port: 6124

    jobmanager.rpc.address: flink-jobmanager.flink.svc.cluster.local
    jobmanager.memory.process.size: 64Gb
    jobmanager.rpc.port: 6123
    queryable-state.proxy.ports: 6125

    metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
    metrics.reporter.prom.interval: 5 SECONDS
    metrics.reporter.prom.port: 9249
    metrics.fetcher.update-interval: 5000
    metrics.latency.granularity: subtask

    state.backend.latency-track.keyed-state-enabled: true
    state.backend.type: rocksdb
    #state.backend.incremental: true

    state.backend.rocksdb.metrics.estimate-num-keys: true
    state.backend.rocksdb.memory.managed: true
    state.backend.rocksdb.async-threads: 4
    state.backend.rocksdb.localdir: /mnt/data/tmp
    state.checkpoint-storage: filesystem
    state.checkpoints.dir: s3://flink-bucket/checkpoints
    state.savepoints.dir: s3://flink-bucket/savepoints
    s3.path.style.access: true
    s3.endpoint: http://minio.minio-operator.svc.cluster.local
    s3.access-key: minio
    s3.secret-key: minio123

    # gives metrics about inbound/outbound network queue lengths
    table.exec.state.ttl: 10 ms
    execution.checkpointing.mode: EXACTLY_ONCE
    execution.checkpointing.unaligned.enabled: true
    execution.checkpointing.aligned-checkpoint-timeout: 10 s
    execution.checkpointing.checkpoints-after-tasks-finish.enabled: true
    cluster.fine-grained-resource-management.enabled: true
    
    pipeline.max-parallelism: 256


  log4j-console.properties: |+
    # This affects logging for both user code and Flink
    rootLogger.level=INFO
    rootLogger.appenderRef.console.ref=ConsoleAppender
    #rootLogger.appenderRef.rolling.ref = RollingFileAppender
    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO
    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name=akka
    logger.akka.level=INFO
    logger.kafka.name=org.apache.kafka
    logger.kafka.level=INFO
    logger.hadoop.name=org.apache.hadoop
    logger.hadoop.level=INFO
    logger.zookeeper.name=org.apache.zookeeper
    logger.zookeeper.level=INFO
    # Log all infos to the console
    appender.console.name=ConsoleAppender
    appender.console.type=CONSOLE
    appender.console.layout.type=PatternLayout
    appender.console.layout.pattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    # Log all infos in the given rolling file
    #appender.rolling.name = RollingFileAppender
    #appender.rolling.type = RollingFile
    #appender.rolling.append = false
    #appender.rolling.fileName = ${sys:log.file}
    #appender.rolling.filePattern = ${sys:log.file}.%i
    #appender.rolling.layout.type = PatternLayout
    #appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    #appender.rolling.policies.type = Policies
    #appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
    #appender.rolling.policies.size.size=100MB
    #appender.rolling.strategy.type = DefaultRolloverStrategy
    #appender.rolling.strategy.max = 10
    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name=org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level=OFF



---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flink-jobmanager
  namespace: flink
spec:
  replicas: 1
  selector:
    matchLabels:
      app: flink
      component: jobmanager
  template:
    metadata:
      labels:
        app: flink
        component: jobmanager
    spec:
      nodeSelector:
        node-role.kubernetes.io/control-plane: "true"
      terminationGracePeriodSeconds: 0
      imagePullSecrets:
        - name: gitlab-scalehub
      containers:
        - name: jobmanager
          image: "{{image}}:{{tag}}"
          env:
            - name: CHECKPOINTING
              value: "{% if checkpoint_interval is defined and checkpoint_interval != '0' %}true{% else %}false{% endif %}"
            - name: CHECKPOINTING_INTERVAL_MS
              value: "{{ checkpoint_interval }}"
            - name: KAFKA_BOOTSTRAP_SERVERS
              value: "kafka-service.kafka.svc.cluster.local:9092"
            - name: SCHEMA_REGISTRY_URL
              value: "http://schema-registry-service.kafka.svc.cluster.local:8081"
            - name: KAFKA_INPUT_TOPIC1
              value: "input-topic1"
            - name: KAFKA_INPUT_TOPIC2
              value: "input-topic2"
            - name: KAFKA_OUTPUT_TOPIC
              value: "output-topic"
            - name: FLINK_STATE_BACKEND
              value: "rocksdb"
            - name: REDIS_HOST
              value: "redis-service"
            - name: ZOOKEEPER_SERVER
              value: "zookeeper-service"
            - name: JOB_WINDOW_SIZE
              value: "{{ window_size }}"
            - name: FIB_INPUT
              value: "{{ fibonacci_value }}"
            - name: UNCHAINED_TASKS
              value: "{{ unchained_tasks }}"
          resources:
            limits:
              memory: 8Gi
              cpu: 4000m
          args: [ "jobmanager" ]
          ports:
            - containerPort: 6123
              name: rpc
            - containerPort: 6124
              name: blob-server
            - containerPort: 8081
              name: webui
            - containerPort: 9249
              name: metrics
          livenessProbe:
            tcpSocket:
              port: 6123
            initialDelaySeconds: 30
            periodSeconds: 60
          volumeMounts:
            - name: flink-config-volume
              mountPath: /opt/flink/conf
            - name: data-volume
              mountPath: /mnt/data
            - name: artifacts-volume
              mountPath: /tmp/jobs
          securityContext:
            runAsUser: 9999  # refers to user _flink_ from official flink image, change if necessary
      volumes:
        - name: flink-config-volume
          configMap:
            name: flink-config-jobmanager
            items:
              - key: flink-conf-jobmanager.yaml
                path: flink-conf.yaml
              - key: log4j-console.properties
                path: log4j-console.properties
        - name: data-volume
          hostPath:
            path: /tmp/experiment-data
        - name: artifacts-volume
          persistentVolumeClaim:
            claimName: artifacts-pvc
