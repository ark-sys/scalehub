#!/usr/bin/env python3
import argparse
# import argcomplete
import json

from src.utils.Logger import Logger
from src.utils.Config import Config
from src.utils.Defaults import ConfigKeys as Key, DefaultValues as Value
from src.utils.Misc import Misc
from src.G5k import G5k
from src.Platform import Platform
from src.Playbooks import Playbooks
from src.Experiment import Experiment


def provision(config: Config):
    platform_type = config.get_str(Key.TYPE)
    match platform_type:
        case "Grid5000":
            # Initiate oarsub request
            log.info("Reserving...")
            p: Platform = G5k(config)
            inventory = p.setup()

            # Dump list of requested nodes to file
            with open(config.get_str(Key.INVENTORY_PATH), "w") as f:
                json.dump(inventory, f, ensure_ascii=False, indent=4)

            # Setup cluster with docker, k3s and appropriate labels
            play: Playbooks = Playbooks()
            return play.deploy("cluster-setup")
        case _:
            log.error(f"Provision is not implemented for platform {platform_type}")


def destroy(config: Config):
    platform_type = config.get_str(Key.TYPE)
    match platform_type:
        case "Grid5000":
            # Initiate oardel request
            p: Platform = G5k(config)
            return p.destroy()
        case _:
            log.error(f"Destroy is not implemented for platform {platform_type}")


def deploy(playbook, config: Config):
    log.info(f"Executing deployment tasks in playbook : {playbook} ...")
    p: Playbooks = Playbooks()
    return p.deploy(playbook, job_file=config.get_str(Key.JOB), task_name=config.get_str(Key.TASK))


def delete(playbook, config: Config):
    log.info(f"Executing deletion tasks in playbook : {playbook} ...")
    p: Playbooks = Playbooks()
    return p.delete(playbook, job_file=config.get_str(Key.JOB))


def run(action, config):
    log.info(f"Running action {action} ...")
    m: Misc = Misc()
    match action:
        case "job":
            return m.run_command(pod_name="flink-jobmanager", command=f"flink run -d -j /tmp/jobs/{config.get_str(Key.JOB)}")
        case "transscale":
            e: Experiment = Experiment(config)
            print(f"starting experiment at {e.start_ts}")
            ret = deploy("transscale", config)
            print(f"Return value from ansible {ret}")
            #TODO trigger experiment completion phase when ret value shows that the job succeded
            print(f"finishing experiment at {e.get_current_timestamp()}")
            # return m.run_command(pod_name="transscale", command="bash -c 'python run_transscale.py -e > /proc/1/fd/0'")
        case _:
            log.error(f"Action {action} is not implemented.")


def export(config: Config):
    log.info("Exporting...")
    #TODO WIP
    # Get the current date in the format DD-MM-YYYY
    # current_date = date.today().strftime('%d-%m-%Y')
    # e.create_exp_folder(current_date)
    # e.get_current_timestamp()
    # TEST QUERY
    # "http://localhost:8428/api/v1/export/csv?match[]=flink_taskmanager_job_task_numRecordsOutPerSecond{task_name=~'TumblingEventTimeWindows'}&start=1688983791726&end=1688994160302&format='__name__,__value__,__timestamp__:__unix_s__'"
    #  curl -o output.csv -d 'format=__name__,task_name,subtask_index,__value__,__timestamp__:unix_s' -d 'match[]=flink_taskmanager_job_task_numRecordsOutPerSecond{task_name=~"TumblingEventTimeWindows"}' -d 'start=1688983791' -d 'end=1688994160' http://localhost:8428/api/v1/export/csv
    #  curl -o output.csv -d 'format=__name__,task_name,subtask_index,__value__,__timestamp__:unix_s' -d 'match[]=sum by(task_name) (flink_taskmanager_job_task_numRecordsOutPerSecond{task_name=~"TumblingEventTimeWindows"})' -d 'start=1688983791' -d 'end=1688994160' http://localhost:8428/api/v1/export/csv
    e: Experiment = Experiment(config)
    e.export_data_to_csv(exp_path="/tmp/test", time_series_name="flink_taskmanager_job_task_numRecordsOutPerSecond",
                         start_timestamp=1688983791726, end_timestamp=1688994160302)


def plot():
    #TODO WIP
    # This metric polls the throughput
    # sum by(task_name) (flink_taskmanager_job_task_numRecordsOutPerSecond{task_name=~"$Operator"})
    # This metric polls the operator parallelism
    # sum by(task_name) (count_values by() ("subtask_index", flink_taskmanager_job_task_numRecordsOutPerSecond{task_name=~"$Operator"}))
    log.info("Plotting...")


def add_time():
    pass


def main():
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest="command")

    parser.add_argument('-c', '--conf', dest='conf_file', action='store',
                        default=Value.System.CONF_PATH,
                        help="Specify a custom path for the configuration file of scalehub.")
    # Provision command
    subparsers.add_parser("provision", help="Provision platform specified in conf/scalehub.conf")

    # Destroy command
    subparsers.add_parser("destroy", help="Destroy platform specified in conf/scalehub.conf")

    # Deploy command
    deploy_parser = subparsers.add_parser("deploy", help="Executes deploy tasks of provided playbook.")
    deploy_parser.add_argument("playbook", help="Name of the playbook.")

    # Delete command
    delete_parser = subparsers.add_parser("delete", help="Executes delete tasks of provided playbook.")
    delete_parser.add_argument("playbook", help="Name of the playbook.")

    # Run command
    run_parser = subparsers.add_parser("run", help="Run action.")
    run_parser.add_argument("action", help="Specify the action to be run. {job | transscale}")

    # Export command
    subparsers.add_parser("export", help="Export data")

    # Plot command
    subparsers.add_parser("plot", help="Plot data")

    # TODO
    # # Setup autocomplete for the script
    # argcomplete.autocomplete(parser)

    # Parse command line arguments
    args = parser.parse_args()

    # Parse configuration file values
    configuration_file = args.conf_file

    # Store configuration file values in Config struct
    config = Config(log, configuration_file)

    if args.command == "provision":
        provision(config)
    elif args.command == "deploy":
        deploy(args.playbook, config)
    elif args.command == "delete":
        delete(args.playbook, config)
    elif args.command == "destroy":
        destroy(config)
    elif args.command == "run":
        run(args.action, config)
    elif args.command == "export":
        export(config)
    elif args.command == "plot":
        plot()
    else:
        parser.print_help()


if __name__ == "__main__":
    log = Logger()
    main()
