#!/usr/bin/env python3
import argparse

# import argcomplete
import os.path
import re
from time import sleep

from src.Experiment import Experiment, ExperimentData
from src.G5k import G5k
from src.Platform import Platform
from src.Playbooks import Playbooks
from src.Plotter import Plotter
from src.utils.Config import Config
from src.utils.Defaults import ConfigKeys as Key, DefaultValues as Value
from src.utils.Logger import Logger
from src.utils.Misc import Misc


# Build cluster and install dependencies
def provision(config: Config):
    platform_type = config.get_str(Key.TYPE)
    match platform_type:
        case "Grid5000":
            # Initiate oarsub request
            log.info("Reserving...")
            p: Platform = G5k(config, log)
            inventory = p.setup()

            # Dump list of requested nodes to file
            with open(config.get_str(Key.INVENTORY_PATH), "w") as f:
                f.write(inventory)

            # Setup cluster with kubernetes
            play: Playbooks = Playbooks()
            kubernetes_type = f"cluster-setup.{config.get_str(Key.KUBERNETES_TYPE)}"
            return play.deploy(kubernetes_type)
        case _:
            log.error(f"Provision is not implemented for platform {platform_type}")


# Destroy cluster
def destroy(config: Config):
    platform_type = config.get_str(Key.TYPE)
    match platform_type:
        case "Grid5000":
            # Initiate oardel request
            p: Platform = G5k(config, log)
            return p.destroy()
        case _:
            log.error(f"Destroy is not implemented for platform {platform_type}")


# Execute tasks with "deploy" tag for a given playbook
def deploy(playbook, config: Config):
    log.info(f"Executing deployment tasks in playbook : {playbook} ...")
    p: Playbooks = Playbooks()
    if playbook == "load_generators":
        for lg_config in config.parse_load_generators():
            p.deploy(
                playbook,
                lg_name=lg_config["name"],
                lg_topic=lg_config["topic"],
                lg_numsensors=int(lg_config["num_sensors"]),
                lg_intervalms=int(lg_config["interval_ms"]),
                lg_replicas=int(lg_config["replicas"]),
                lg_value=int(lg_config["value"]),
            )
    elif playbook == "transscale":
        p.deploy(
            playbook,
            job_file=config.get_str(Key.JOB),
            task_name=config.get_str(Key.TASK),
            max_parallelism=config.get_int(Key.TRANSCCALE_PAR),
            warmup=config.get_int(Key.TRANSSCALE_WARMUP),
            interval=config.get_int(Key.TRANSSCALE_INTERVAL),
        )
    elif playbook == "chaos":
        p.deploy(
            playbook,
            kubernetes_type=config.get_str(Key.KUBERNETES_TYPE),
        )
    else:
        return p.deploy(
            playbook,
            job_file=config.get_str(Key.JOB),
            task_name=config.get_str(Key.TASK),
        )


# Execute tasks with "delete" tag for a given playbook
def delete(playbook, config: Config):
    log.info(f"Executing deletion tasks in playbook : {playbook} ...")
    p: Playbooks = Playbooks()
    if playbook == "load_generators":
        for lg_config in config.parse_load_generators():
            p.delete(
                playbook,
                lg_name=lg_config["name"],
                lg_topic=lg_config["topic"],
                lg_numsensors=int(lg_config["num_sensors"]),
                lg_intervalms=int(lg_config["interval_ms"]),
                lg_replicas=int(lg_config["replicas"]),
                lg_value=int(lg_config["value"]),
            )
    elif playbook == "transscale":
        p.delete(
            playbook,
            job_file=config.get_str(Key.JOB),
            task_name=config.get_str(Key.TASK),
            max_parallelism=config.get_int(Key.TRANSCCALE_PAR),
            warmup=config.get_int(Key.TRANSSCALE_WARMUP),
            interval=config.get_int(Key.TRANSSCALE_INTERVAL),
        )
    elif playbook == "chaos":
        p.delete(
            playbook,
            kubernetes_type=config.get_str(Key.KUBERNETES_TYPE),
        )
    else:
        return p.delete(playbook, job_file=config.get_str(Key.JOB))


# Delete and re-deploy 'playbook' parameter
def reload(playbook, config: Config):
    # Delete running playbook
    delete(playbook, config)
    # Give some time for tasks to execute
    sleep(5)
    # Deploy again playbook
    deploy(playbook, config)


# Quick action commands
def run(action, config):
    log.info(f"Running action {action} ...")
    m: Misc = Misc(log)
    match action:
        case "job":
            return m.execute_command_on_pod(
                deployment_name="flink-jobmanager",
                command=f"flink run -d -j /tmp/jobs/{config.get_str(Key.JOB)}",
            )
        case "transscale":
            e: Experiment = Experiment(config, log)
            e.transscale_only_run()
        case "experiment":
            e: Experiment = Experiment(config, log)
            e.full_run()
            reload("flink", config)
        case "chaos":
            m.apply_kubernetes_resource(
                "/app/playbooks/project/roles/chaos/files/chaos-experiment.yaml"
            )
        case "clean":
            # Clean flink jobs
            m.execute_command_on_pod(
                deployment_name="flink-jobmanager",
                command="for job_id in $(flink list -r | awk -F ' : ' '/\(RUNNING\)/ {print $2}'); do flink cancel $job_id ;done",
            )

            # Scale down taskmanagers
            m.scale_deployment("flink-taskmanager")
            # Clean transscale job
            m.delete_job("transscale-job")
        case "test":
            pass
        case _:
            log.error(f"Action {action} is not implemented.")


# Export data for a given date
def export(config: Config, exp_path: str):
    log.info("Exporting...")

    def export_experiment(path):
        # Verify that a log file exists for the chosen experiment
        log_path = os.path.join(path, "exp_log.txt")
        if not os.path.exists(log_path):
            log.error("Log file for experiment not found.")
            return
        else:
            # Verify if experiment executed correctly by checking start and end timestamps
            with open(log_path, "r") as log_file:
                logs = log_file.read()

            start_match = re.search(r"Experiment start at : (\d+)", logs)
            end_match = re.search(r"Experiment end at : (\d+)", logs)
            if start_match and end_match:
                start_timestamp = int(start_match.group(1))
                end_timestamp = int(end_match.group(1))
                data: ExperimentData = ExperimentData(
                    log=log,
                    exp_path=path,
                    start_ts=start_timestamp,
                    end_ts=end_timestamp,
                    db_url=config.get_str(Key.DB_URL),
                )
                data.export_experiment_data()
                stats, _ = data.eval_stats(
                    skip_duration=config.get_int(Key.DATA_SKIP_DURATION)
                )
                if config.get_bool(Key.DATA_OUTPUT_PLOT):
                    data.eval_plot(stats)
                return
            else:
                log.error("Log file is incomplete: missing timestamp.")

    # Generate full path from provided path name
    # exp_path should be 'DD-MM-YYYY/N' where or 'DD-MM-YYYY' :
    #   DD-MM-YYYY is the date of the experiments
    #   N is the number of the experiment

    # Define regular expression patterns for 'DD-MM-YYYY' and 'DD-MM-YYYY/N' formats
    exact_format_pattern = r"^\d{2}-\d{2}-\d{4}$"
    with_integer_pattern = r"^\d{2}-\d{2}-\d{4}/(\d+)$"

    # Try to match the 'DD-MM-YYYY' format
    exact_match = re.match(exact_format_pattern, exp_path)
    # If it doesn't match the exact format, try the 'DD-MM-YYYY/N' format
    with_integer_match = re.match(with_integer_pattern, exp_path)

    if exact_match:
        base_exp_path = os.path.join(
            config.get_str(Key.EXPERIMENTS_DATA_PATH), exp_path
        )
        log.info(f"Exporting experiments for {base_exp_path}.")
        # Iterate through each experiment folder of the base exp path
        for exp in os.listdir(base_exp_path):
            full_exp_path = os.path.join(base_exp_path, exp)
            export_experiment(full_exp_path)
    elif with_integer_match:
        log.info(f"Exporting {exp_path} experiment.")
        full_exp_path = os.path.join(
            config.get_str(Key.EXPERIMENTS_DATA_PATH), exp_path
        )
        export_experiment(full_exp_path)
    else:
        log.error("Error during export.")


# Start GUI for plot generation
def plot(config: Config):
    log.info("Starting plotter.")
    p: Plotter = Plotter(config, log)


def main(log: Logger):
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest="command")

    parser.add_argument(
        "-c",
        "--conf",
        dest="conf_file",
        action="store",
        default=Value.System.CONF_PATH,
        help="Specify a custom path for the configuration file of scalehub.",
    )
    # Provision command
    subparsers.add_parser(
        "provision", help="Provision platform specified in conf/scalehub.conf"
    )

    # Destroy command
    subparsers.add_parser(
        "destroy", help="Destroy platform specified in conf/scalehub.conf"
    )

    # Deploy command
    deploy_parser = subparsers.add_parser(
        "deploy", help="Executes deploy tasks of provided playbook."
    )
    deploy_parser.add_argument("playbook", help="Name of the playbook.")

    # Delete command
    delete_parser = subparsers.add_parser(
        "delete", help="Executes delete tasks of provided playbook."
    )
    delete_parser.add_argument("playbook", help="Name of the playbook.")

    # Reload command
    reload_parser = subparsers.add_parser(
        "reload", help="Executes reload tasks of provided playbook."
    )
    reload_parser.add_argument("playbook", help="Name of the playbook.")

    # Run command
    run_parser = subparsers.add_parser("run", help="Run action.")
    run_parser.add_argument(
        "action", help="Specify the action to be run. {job | transscale}"
    )

    # Export command
    export_subparser = subparsers.add_parser("export", help="Export data")
    export_subparser.add_argument(
        "experiment", help="Provide experiment path in format 'DD-MM-YYYY/N'"
    )

    # Plot command
    subparsers.add_parser("plot", help="Starts interactive data plotter.")

    # Parse command line arguments
    args = parser.parse_args()

    # Parse configuration file values
    configuration_file = args.conf_file

    # Store configuration file values in Config struct
    config = Config(log, configuration_file)

    if args.command == "provision":
        provision(config)
    elif args.command == "deploy":
        deploy(args.playbook, config)
    elif args.command == "delete":
        delete(args.playbook, config)
    elif args.command == "reload":
        reload(args.playbook, config)
    elif args.command == "destroy":
        destroy(config)
    elif args.command == "run":
        run(args.action, config)
    elif args.command == "export":
        export(config, args.experiment)
    elif args.command == "plot":
        plot(config)
    else:
        parser.print_help()


if __name__ == "__main__":
    log = Logger()
    main(log)
